<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Tianrong  Chen


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üî•</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-XXXXXXXXX');
</script>



<!-- Panelbear Analytics - We respect your privacy -->
<script async src="https://cdn.panelbear.com/analytics.js?site=XXXXXXXXX"></script>
<script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: 'XXXXXXXXX' });
</script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Tianrong</span>  Chen
    </h1>
     <p class="desc"><p>Contact:tchen429 [at] gatech [dot] edu</p></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/selfee.JPG">
      
      
        <div class="address">
          <p>Room 405, Montgomery Knight Building</p> <p>270 Ferst Drive, Georgia Institute of Technology</p> <p>Atlanta, GA 30332-0150</p>

        </div>
      
    </div>
    

    <div class="clearfix">
      <p>Hi there, I am a senior PhD in Autonomous Control Decision System (ACDS) Lab at Georiga Institute of Technology advised by Professor <a href="https://ae.gatech.edu/people/evangelos-theodorou">Evangelos Theodorou</a>.</p>

<p>My research interests are Generative Model, Stochastic Optimal Control, and Large-Scale Optimization.</p>

<p>I am dedicating in improving deep learning models by leveraging Stochastic Optimal Control (SOC) theory. I am also interested in solving large-scale problem by utilizing the magic power of deep learning.</p>


    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">May 8, 2023</th>
          <td>
            
              I join Apple MLR as Research Scientist Intern supervised by <a href="https://www.linkedin.com/in/shuangfei-zhai/">Shuangfei Zhai</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 8, 2023</th>
          <td>
            
              I am looking for the full-time job starting from Fall 2024!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 8, 2023</th>
          <td>
            
              Our paper Multi-Marginal <a href="https://arxiv.org/abs/2303.01751">Schrodinger Bridge</a> and <a href="https://arxiv.org/pdf/2310.01236v1.pdf">Mirror Diffusion Models for Constrained and Watermarked Generation</a> are accepted in NeurIPS2023!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 8, 2023</th>
          <td>
            
              Our paper Multi-Marginal <a href="https://arxiv.org/abs/2303.01751">Schrodinger Bridge</a> and <a href="https://arxiv.org/pdf/2310.01236v1.pdf">Mirror Diffusion Models for Constrained and Watermarked Generation</a> are accepted in NeurIPS2023!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 20, 2022</th>
          <td>
            
              Our paper <a href="https://arxiv.org/abs/2209.09893">deepGSB</a>, accepted to NeruIPS 2022!

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>Publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS 2023</abbr>
    
  
  </div>

  <div id="chen2023deep" class="col-sm-8">
    
      <div class="title">Deep Momentum Multi-Marginal Schr√∂dinger Bridge</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Chen, Tianrong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Guan-Horng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tao, Molei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Theodorou, Evangelos A
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2303.01751</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2303.01751.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>It is a crucial challenge to reconstruct population dynamics using unlabeled samples from distributions at coarse time intervals. Recent approaches such as flow-based models or Schr√∂dinger Bridge models have demonstrated appealing performance, yet the inferred sample trajectories either fail to account for the underlying stochasticity or are unnecessarily rigid. In this article, We extend the approach in [1] to operate in continuous space and propose Deep Momentum Multi-Marginal Schr√∂dinger Bridge (DMSB), a novel computational framework that learns the smooth measure-valued spline for stochastic systems that satisfy position marginal constraints across time. By tailoring the celebrated Bregman Iteration and extending the Iteration Proportional Fitting to phase space, we manage to handle high-dimensional multi-marginal trajectory inference tasks efficiently. Our algorithm outperforms baselines significantly, as evidenced by experiments for synthetic datasets and a real-world single-cell RNA sequence dataset. Additionally, the proposed approach can reasonably reconstruct the evolution of velocity distribution, from position snapshots only, when there is a ground truth velocity that is nevertheless inaccessible.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chen2023deep</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{NeurIPS 2023}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Momentum Multi-Marginal Schr\"odinger Bridge}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Tianrong and Liu, Guan-Horng and Tao, Molei and Theodorou, Evangelos A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2303.01751}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2303.01751.pdf}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS 2023</abbr>
    
  
  </div>

  <div id="chen2023deeq" class="col-sm-8">
    
      <div class="title">Mirror Diffusion Models for Constrained and Watermarked Generation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Liu, Guan-Horng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Chen, Tianrong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tao, Molei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Theodorou, Evangelos A
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2310.01236v1</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2310.01236v1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Modern successes of diffusion models in learning complex, high-dimensional data distributions are attributed, in part, to their capability to construct diffusion processes with analytic transition kernels and score functions. The tractability results in a simulation-free framework with stable regression losses, from which reversed, generative processes can be learned at scale. However, when data is confined to a constrained set as opposed to a standard Euclidean space, these desirable characteristics appear to be lost based on prior attempts. In this work, we propose Mirror Diffusion Models (MDM), a new class of diffusion models that generate data on convex constrained sets without losing any tractability. This is achieved by learning diffusion processes in a dual space constructed from a mirror map, which, crucially, is a standard Euclidean space. We derive efficient computation of mirror maps for popular constrained sets, such as simplices and ‚Ñì2-balls, showing significantly improved performance of MDM over existing methods. For safety and privacy purposes, we also explore constrained sets as a new mechanism to embed invisible but quantitative information (i.e., watermarks) in generated data, for which MDM serves as a compelling approach. Our work brings new algorithmic opportunities for learning tractable diffusion on complex domains.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chen2023deeq</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{NeurIPS 2023}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mirror Diffusion Models for Constrained and Watermarked Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Guan-Horng and Chen, Tianrong and Tao, Molei and Theodorou, Evangelos A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2310.01236v1}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2310.01236v1.pdf}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS 2022</abbr>
    
  
  </div>

  <div id="liu2022deep" class="col-sm-8">
    
      <div class="title">Deep Generalized Schr√∂dinger Bridge</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Liu, Guan-Horng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Chen, Tianrong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  So, Oswin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Theodorou, Evangelos A
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2209.09893</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/abs/2209.09893" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Mean-Field Game (MFG) serves as a crucial mathematical framework in modeling the collective behavior of individual agents interacting stochastically with a large population. In this work, we aim at solving a challenging class of MFGs in which the differentiability of these interacting preferences may not be available to the solver, and the population is urged to converge exactly to some desired distribution. These setups are, despite being well-motivated for practical purposes, complicated enough to paralyze most (deep) numerical solvers. Nevertheless, we show that Schr√∂dinger Bridge - as an entropy-regularized optimal transport model - can be generalized to accepting mean-field structures, hence solving these MFGs. This is achieved via the application of Forward-Backward Stochastic Differential Equations theory, which, intriguingly, leads to a computational framework with a similar structure to Temporal Difference learning. As such, it opens up novel algorithmic connections to Deep Reinforcement Learning that we leverage to facilitate practical training. We show that our proposed objective function provides necessary and sufficient conditions to the mean-field problem. Our method, named Deep Generalized Schr√∂dinger Bridge (DeepGSB), not only outperforms prior methods in solving classical population navigation MFGs, but is also capable of solving 1000-dimensional opinion depolarization, setting a new state-of-the-art numerical solver for high-dimensional MFGs.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2022deep</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{NeurIPS 2022}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Generalized Schr\"odinger Bridge}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Guan-Horng and Chen, Tianrong and So, Oswin and Theodorou, Evangelos A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2209.09893}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2209.09893}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CDC 2022</abbr>
    
  
  </div>

  <div id="chen2022deep" class="col-sm-8">
    
      <div class="title">Deep Graphic FBSDEs for Opinion Dynamics Stochastic Control</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Chen, Tianrong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wang, Ziyi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Theodorou, Evangelos A
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2204.02506</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/abs/2204.02506" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we present a scalable deep learning approach to solve opinion dynamics stochastic optimal control problems with mean field term coupling in the dynamics and cost function. Our approach relies on the probabilistic representation of the solution of the Hamilton-Jacobi-Bellman partial differential equation. Grounded on the nonlinear version of the Feynman-Kac lemma, the solutions of the Hamilton-Jacobi-Bellman partial differential equation are linked to the solution of Forward-Backward Stochastic Differential Equations. These equations can be solved numerically using a novel deep neural network with architecture tailored to the problem in consideration. The resulting algorithm is tested on a polarized opinion consensus experiment. The large-scale (10K) agents experiment validates the scalability and generalizability of our algorithm. The proposed framework opens up the possibility for future applications on extremely large-scale problems.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chen2022deep</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CDC 2022}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Graphic FBSDEs for Opinion Dynamics Stochastic Control}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Tianrong and Wang, Ziyi and Theodorou, Evangelos A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2204.02506}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2204.02506}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR 2022</abbr>
    
  
  </div>

  <div id="chen2021likelihood" class="col-sm-8">
    
      <div class="title">Likelihood Training of Schr√∂dinger Bridge using Forward-Backward SDEs Theory</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Chen, Tianrong*</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Guan-Horng*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Theodorou, Evangelos A
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2110.11291</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2110.11291.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Schr√∂dinger Bridge (SB) is an optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely on constructing parameterized log-likelihood objectives.This raises questions on the suitability of SB models as a principled alternative for generative applications. In this work, we present a novel computational framework for likelihood training of SB models grounded on Forward-Backward Stochastic Differential Equations Theory ‚Äì a mathematical methodology appeared in stochastic optimal control that transforms the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be used to construct the likelihood objectives for SB that, surprisingly, generalizes the ones for SGM as special cases. This leads to a new optimization principle that inherits the same SB optimality yet without losing applications of modern generative training techniques, and we show that the resulting training algorithm achieves comparable results on generating realistic images on MNIST, CelebA, and CIFAR10.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chen2021likelihood</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICLR 2022}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Likelihood Training of Schr\"odinger Bridge using Forward-Backward SDEs Theory}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Tianrong* and Liu, Guan-Horng* and Theodorou, Evangelos A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2110.11291}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2110.11291.pdf}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeuraIPS 2021</abbr>
    
  
  </div>

  <div id="liu2021second" class="col-sm-8">
    
      <div class="title">Second-Order Neural ODE Optimizer</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Liu, Guan-Horng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Chen, Tianrong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Theodorou, Evangelos A
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2109.14158</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/https://arxiv.org/abs/2105.03788" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2109.14158.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a novel second-order optimization framework for training the emerging deep continuous-time models, specifically the Neural Ordinary Differential
Equations (Neural ODEs). Since their training already involves expensive gradient
computation by solving a backward ODE, deriving efficient second-order methods
becomes highly nontrivial. Nevertheless, inspired by the recent Optimal Control
(OC) interpretation of training deep networks, we show that a specific continuoustime OC methodology, called Differential Programming, can be adopted to derive
backward ODEs for higher-order derivatives at the same O(1) memory cost. We
further explore a low-rank representation of the second-order derivatives and show
that it leads to efficient preconditioned updates with the aid of Kronecker-based
factorization. The resulting method converges much faster than first-order baselines
in wall-clock time, and the improvement remains consistent across various applications, e.g. image classification, generative flow, and time-series prediction. Our
framework also enables direct architecture optimization, such as the integration
time of Neural ODEs, with second-order feedback policies, strengthening the OC
perspective as a principled tool of analyzing optimization in deep learning.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2021second</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{NeuraIPS 2021}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Second-Order Neural ODE Optimizer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Guan-Horng and Chen, Tianrong and Theodorou, Evangelos A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2109.14158}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2109.14158.pdf}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2105.03788}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML 2021</abbr>
    
  
  </div>

  <div id="liu2021dynamic" class="col-sm-8">
    
      <div class="title">Dynamic Game Theoretic Neural Optimizer</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Liu, Guan-Horng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Chen, Tianrong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Theodorou, Evangelos A
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2105.03788</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/https://arxiv.org/abs/2105.03788" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2105.03788.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
      
      <a href="https://docs.google.com/presentation/d/1Pm-5h_0jji6ocsarzhdjXZpBpTtwr9kx44fyyU67hH4/edit?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="https://icml.cc/media/icml-2021/Slides/10261.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The connection between training deep neural networks (DNNs) and optimal control theory (OCT) has attracted considerable attention as a principled tool of algorithmic design. Despite few attempts being made, they have been limited to architectures where the layer propagation resembles a Markovian dynamical system. This casts doubts on their flexibility to modern networks that heavily rely on non-Markovian dependencies between layers (e.g. skip connections in residual networks). In this work, we propose a novel dynamic game perspective by viewing each layer as a player in a dynamic game characterized by the DNN itself. Through this lens, different classes of optimizers can be seen as matching different types of Nash equilibria, depending on the implicit information structure of each (p)layer. The resulting method, called Dynamic Game Theoretic Neural Optimizer (DGNOpt), not only generalizes OCT-inspired optimizers to richer network class; it also motivates a new training principle by solving a multi-player cooperative game. DGNOpt shows convergence improvements over existing methods on image classification datasets with residual and inception networks. Our work marries strengths from both OCT and game theory, paving ways to new algorithmic opportunities from robust optimal control and bandit-based optimization.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2021dynamic</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICML 2021}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Game Theoretic Neural Optimizer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Guan-Horng and Chen, Tianrong and Theodorou, Evangelos A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2105.03788}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2105.03788}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2105.03788.pdf}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{https://docs.google.com/presentation/d/1Pm-5h_0jji6ocsarzhdjXZpBpTtwr9kx44fyyU67hH4/edit?usp=sharing}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{https://icml.cc/media/icml-2021/Slides/10261.pdf}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://icml.cc/virtual/2021/poster/10261}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML 2021</abbr>
    
  
  </div>

  <div id="chen2021large" class="col-sm-8">
    
      <div class="title">Large-Scale Multi-Agent Deep FBSDEs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Chen, Tianrong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wang, Ziyi O,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Exarchos, Ioannis,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Theodorou, Evangelos
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/https://arxiv.org/abs/2011.10890" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2011.10890.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
      
      <a href="https://github.com/TianrongChen/SFP-FBSDE/blob/main/ICML2021_FP_FBSDE_Poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="https://github.com/TianrongChen/SFP-FBSDE/blob/main/_ICML2021__Presentation_large_Scale_Multi_Agent_Deep_FBSDEs.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper we present a scalable deep learning framework for finding Markovian Nash Equilibria in multi-agent stochastic games using fictitious play. The motivation is inspired by theoretical analysis of Forward Backward Stochastic Differential Equations (FBSDE) and their implementation in a deep learning setting, which is the source of our algorithm‚Äôs sample efficiency improvement. By taking advantage of the permutation-invariant property of agents in symmetric games, the scalability and performance is further enhanced significantly. We showcase superior performance of our framework over the state-of-the-art deep fictitious play algorithm on an inter-bank lending/borrowing problem in terms of multiple metrics. More importantly, our approach scales up to 3000 agents in simulation, a scale which, to the best of our knowledge, represents a new state-of-the-art. We also demonstrate the applicability of our framework in robotics on a belief space autonomous racing problem.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen2021large</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Large-Scale Multi-Agent Deep FBSDEs}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICML 2021}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Tianrong and Wang, Ziyi O and Exarchos, Ioannis and Theodorou, Evangelos}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1740--1748}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2011.10890}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2011.10890.pdf}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{https://github.com/TianrongChen/SFP-FBSDE/blob/main/ICML2021_FP_FBSDE_Poster.pdf}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{https://github.com/TianrongChen/SFP-FBSDE/blob/main/_ICML2021__Presentation_large_Scale_Multi_Agent_Deep_FBSDEs.pdf}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://slideslive.com/38958966/largescale-multiagent-deep-fbsdes?ref=search}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR 2021</abbr>
    
  
  </div>

  <div id="liu2020ddpnopt" class="col-sm-8">
    
      <div class="title">DDPNOpt: Differential Dynamic Programming Neural Optimizer</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Liu, Guan-Horng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Chen, Tianrong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Theodorou, Evangelos A
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2002.08809</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/https://arxiv.org/abs/2002.08809" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2002.08809.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
      
      <a href="https://docs.google.com/presentation/d/1OBW4bXZMDp27smv0za15EjnOV07jC1TpIwA0ww-vlyM/edit?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="https://drive.google.com/file/d/1hCMEGjE5Zt6WYN4TUAAsvj9QlxhsTlHG/view" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2020ddpnopt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DDPNOpt: Differential Dynamic Programming Neural Optimizer}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICLR 2021}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Guan-Horng and Chen, Tianrong and Theodorou, Evangelos A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2002.08809}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2002.08809}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2002.08809.pdf}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{https://docs.google.com/presentation/d/1OBW4bXZMDp27smv0za15EjnOV07jC1TpIwA0ww-vlyM/edit?usp=sharing}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/1hCMEGjE5Zt6WYN4TUAAsvj9QlxhsTlHG/view}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://iclr.cc/virtual/2021/spotlight/3512}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">L4DC 2020</abbr>
    
  
  </div>

  <div id="pereira2020feynman" class="col-sm-8">
    
      <div class="title">Feynman-Kac Neural Network Architectures for Stochastic Control Using Second-Order FBSDE Theory</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Marcus, Pereira*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ziyi, Wang*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tianrong, Chen*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Reed, Emily,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Theodorou, Evangelos
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Learning for Dynamics and Control</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="http://proceedings.mlr.press/v120/pereira20a/pereira20a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a deep recurrent neural network architecture to solve a class of stochastic optimal
control problems described by fully nonlinear Hamilton Jacobi Bellman partial differential equations.
Such PDEs arise when considering stochastic dynamics characterized by uncertainties that are
additive, state dependent, and control multiplicative. Stochastic models with these characteristics
are important in computational neuroscience, biology, finance, and aerospace systems and provide
a more accurate representation of actuation than models with only additive uncertainty. Previous
literature has established the inadequacy of the linear HJB theory for such problems, so instead,
methods relying on the generalized version of the Feynman-Kac lemma have been proposed resulting
in a system of second-order Forward-Backward SDEs. However, so far, these methods suffer from
compounding errors resulting in lack of scalability. In this paper, we propose a deep learning based
algorithm that leverages the second-order FBSDE representation and LSTM-based recurrent neural
networks to not only solve such stochastic optimal control problems but also overcome the problems
faced by traditional approaches, including scalability. The resulting control algorithm is tested on
a high-dimensional linear system and three nonlinear systems from robotics and biomechanics in
simulation to demonstrate feasibility and out-performance against previous methods.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pereira2020feynman</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Feynman-Kac Neural Network Architectures for Stochastic Control Using Second-Order FBSDE Theory}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marcus, Pereira* and Ziyi, Wang* and Tianrong, Chen* and Reed, Emily and Theodorou, Evangelos}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Learning for Dynamics and Control}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{728--738}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{L4DC 2020}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{http://proceedings.mlr.press/v120/pereira20a/pereira20a.pdf}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li></ol>
</div>


    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%74%63%68%65%6E%34%32%39@%67%61%74%65%63%68.%65%64%75"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=r9D3Fg50gMoC&hl=zh-CN#d=gs_hdr_drw" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/TianrongChen" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/Tianrong Chen" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>













      </div>
      <div class="contact-note">Contact preference: Email.
</div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2023 Tianrong  Chen.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
    
    Last updated: October 10, 2023.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
